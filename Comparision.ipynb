{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Richraj14/EdgeModels/blob/main/Comparision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c4IAdzlzJ8-",
        "outputId": "1879d499-9700-468b-eaee-152b21dae55e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDXV6CMq1qNN",
        "outputId": "91068cf1-ecc5-44d7-a61b-83bb4f18c60a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qClVlh0duvVf",
        "outputId": "14a77422-75dd-4bbe-b238-73397a716b89"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.11/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hrzand0KvJbG",
        "outputId": "f537ec9f-2657-4578-915c-3c748f51b9c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.86-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.86-py3-none-any.whl (922 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.86 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from thop import profile  # FLOPs Calculation\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLO Model for Edge AI\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw8hAe98vjcn",
        "outputId": "6c0d43c0-7edb-452f-cdcd-311668f18ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEsUTLJUvqTG",
        "outputId": "e3367e36-7b87-4084-9487-8972c6ec4f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Load Pascal VOC Classification Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "4w3mOoTyvstc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torchvision.datasets.VOCSegmentation(  # Use segmentation for consistent sizes\n",
        "    root=\"./data\", year=\"2012\", image_set=\"val\", download=True, transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=lambda x: (torch.stack([i[0] for i in x]), None))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cut4HBUjvwbx",
        "outputId": "bc76868a-b264-4e7c-90ef-41161953f719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to ./data/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.00G/2.00G [01:50<00:00, 18.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Models to evaluate (EDGE + HIGH-PERFORMANCE)\n",
        "models_to_test = [\n",
        "    \"mobilenet_v3_large\", \"efficientnet_b0\", \"squeezenet1_1\", \"shufflenet_v2_x0_5\",  # EDGE AI\n",
        "    \"resnet50\", \"vit_b_16\", \"convnext_tiny\",  # HIGH-PERFORMANCE\n",
        "]\n"
      ],
      "metadata": {
        "id": "HK5TgpboyWMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Function to measure GPU memory usage\n",
        "def get_memory_usage():\n",
        "    return torch.cuda.memory_allocated(device) / 1e6 if torch.cuda.is_available() else 0\n"
      ],
      "metadata": {
        "id": "ewaZbInGybTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ✅ Function to evaluate a model\n",
        "def evaluate_model(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name} on Pascal VOC...\")\n",
        "\n",
        "    # ✅ Load pre-trained model\n",
        "    if \"yolo\" in model_name:\n",
        "        model = YOLO(\"yolov8n.pt\").to(device)  # Load YOLO-Nano for edge AI\n",
        "    else:\n",
        "        model = getattr(torchvision.models, model_name)(pretrained=True).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # ✅ Measure inference time\n",
        "    total_times = []\n",
        "    for _ in range(num_trials):\n",
        "        dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Adjust input size as needed\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = torch.cuda.Event(enable_timing=True)\n",
        "        end_time = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start_time.record()\n",
        "        with torch.no_grad():\n",
        "            model(dummy_input)\n",
        "        end_time.record()\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        total_times.append(start_time.elapsed_time(end_time) / 1000)  # Convert ms to seconds\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "    throughput = num_trials / sum(total_times) if sum(total_times) > 0 else 0\n",
        "\n",
        "    # ✅ Track VRAM Usage\n",
        "    max_vram = torch.cuda.max_memory_allocated() / (1024**2) if torch.cuda.is_available() else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": \"Edge AI\" if model_name in [\"mobilenet_v3_large\", \"efficientnet_b0\", \"squeezenet1_1\", \"shufflenet_v2_x0_5\", \"yolov8n\"] else \"High-Performance\",\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all models\n",
        "models_to_test = [\"mobilenet_v3_large\", \"efficientnet_b0\", \"squeezenet1_1\", \"shufflenet_v2_x0_5\", \"yolov8n\"]\n",
        "results = [evaluate_model(model) for model in models_to_test]\n",
        "\n",
        "# ✅ Print results\n",
        "for res in results:\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "A61Xki2fyn_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa768b6-0ba5-48e9-e958-016d9a227d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating mobilenet_v3_large on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating efficientnet_b0 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 77.6MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to /root/.cache/torch/hub/checkpoints/squeezenet1_1-b8a52dc0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating squeezenet1_1 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.73M/4.73M [00:00<00:00, 45.0MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ShuffleNet_V2_X0_5_Weights.IMAGENET1K_V1`. You can also use `weights=ShuffleNet_V2_X0_5_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth\" to /root/.cache/torch/hub/checkpoints/shufflenetv2_x0.5-f707e7126e.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating shufflenet_v2_x0_5 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.28M/5.28M [00:00<00:00, 31.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating yolov8n on Pascal VOC...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6.25M/6.25M [00:00<00:00, 109MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.646906852722168. Dividing input by 255.\n",
            "0: 224x224 (no detections), 67.4ms\n",
            "Speed: 0.1ms preprocess, 67.4ms inference, 38.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.4772162437438965. Dividing input by 255.\n",
            "0: 224x224 (no detections), 7.3ms\n",
            "Speed: 0.0ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.210418701171875. Dividing input by 255.\n",
            "0: 224x224 (no detections), 6.9ms\n",
            "Speed: 0.0ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.704095363616943. Dividing input by 255.\n",
            "0: 224x224 (no detections), 6.9ms\n",
            "Speed: 0.0ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.7052741050720215. Dividing input by 255.\n",
            "0: 224x224 (no detections), 7.0ms\n",
            "Speed: 0.0ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 224)\n",
            "{'Model': 'mobilenet_v3_large', 'Category': 'Edge AI', 'Avg Inference Time (s)': 0.0068, 'Min Inference Time (s)': 0.0063, 'Max Inference Time (s)': 0.0078, 'FPS': 147.48, 'Throughput (images/sec)': 147.48, 'Max VRAM Usage (MB)': 58.34}\n",
            "{'Model': 'efficientnet_b0', 'Category': 'Edge AI', 'Avg Inference Time (s)': 0.0235, 'Min Inference Time (s)': 0.0081, 'Max Inference Time (s)': 0.0731, 'FPS': 42.57, 'Throughput (images/sec)': 42.57, 'Max VRAM Usage (MB)': 58.34}\n",
            "{'Model': 'squeezenet1_1', 'Category': 'Edge AI', 'Avg Inference Time (s)': 0.0247, 'Min Inference Time (s)': 0.0034, 'Max Inference Time (s)': 0.1014, 'FPS': 40.48, 'Throughput (images/sec)': 40.48, 'Max VRAM Usage (MB)': 58.34}\n",
            "{'Model': 'shufflenet_v2_x0_5', 'Category': 'Edge AI', 'Avg Inference Time (s)': 0.0217, 'Min Inference Time (s)': 0.0065, 'Max Inference Time (s)': 0.0804, 'FPS': 46.16, 'Throughput (images/sec)': 46.16, 'Max VRAM Usage (MB)': 58.34}\n",
            "{'Model': 'yolov8n', 'Category': 'Edge AI', 'Avg Inference Time (s)': 0.1148, 'Min Inference Time (s)': 0.0112, 'Max Inference Time (s)': 0.528, 'FPS': 8.71, 'Throughput (images/sec)': 8.71, 'Max VRAM Usage (MB)': 58.34}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Compute final metrics\n",
        "  avg_time = np.mean(total_times)\n",
        "  min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "  fps = 1 / avg_time if avg_time > 0 else 0\n",
        "  throughput = len(dataset) / sum(total_times) if sum(total_times) > 0 else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": \"Edge AI\" if model_name in [\"mobilenet_v3_large\", \"efficientnet_b0\", \"squeezenet1_1\", \"shufflenet_v2_x0_5\", \"yolov8n\"] else \"High-Performance\",\n",
        "        \"Model Size (MB)\": round(model_size, 2),\n",
        "        \"FLOPs (G)\": round(macs / 1e9, 2),\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all models\n",
        "results = []\n",
        "for model_name in models_to_test:\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "e-S914Tuysxa",
        "outputId": "91b12698-2925-425c-c75c-cae9ee588175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-18-894c94dfe29c>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-894c94dfe29c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    avg_time = np.mean(total_times)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from thop import profile  # FLOPs Calculation\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLO Model for Edge AI\n",
        "\n",
        "# ✅ Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ Load Pascal VOC Classification Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.VOCSegmentation(  # Use segmentation for consistent sizes\n",
        "    root=\"./data\", year=\"2012\", image_set=\"val\", download=True, transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=lambda x: (torch.stack([i[0] for i in x]), None))\n",
        "\n",
        "# ✅ Models to evaluate (EDGE + HIGH-PERFORMANCE)\n",
        "models_to_test = [\n",
        "    \"mobilenet_v3_large\", \"efficientnet_b0\", \"squeezenet1_1\", \"shufflenet_v2_x0_5\",  # EDGE AI\n",
        "    \"resnet50\", \"vit_b_16\", \"convnext_tiny\",  # HIGH-PERFORMANCE\n",
        "]\n",
        "\n",
        "# ✅ Function to measure GPU memory usage\n",
        "def get_memory_usage():\n",
        "    return torch.cuda.memory_allocated(device) / 1e6 if torch.cuda.is_available() else 0\n",
        "\n",
        "# ✅ Function to evaluate a model\n",
        "def evaluate_model(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name} on Pascal VOC...\")\n",
        "\n",
        "    # ✅ Load pre-trained model\n",
        "    if \"yolo\" in model_name:\n",
        "        model = YOLO(\"yolov8n.pt\").to(device)  # Load YOLO-Nano for edge AI\n",
        "    else:\n",
        "        model = getattr(torchvision.models, model_name)(pretrained=True).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # ✅ Compute FLOPs and Model Size\n",
        "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "    macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "\n",
        "    # ✅ Model size in MB\n",
        "    model_size = (sum(p.numel() for p in model.parameters()) * 4) / (1024 ** 2)\n",
        "\n",
        "    # ✅ Performance tracking\n",
        "    total_times = []\n",
        "    max_vram = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_trials):\n",
        "            for images, _ in tqdm(dataloader, desc=f\"Running {model_name}\", leave=False):\n",
        "                images = images.to(device)\n",
        "\n",
        "                # ✅ Measure inference time\n",
        "                start_time = time.time()\n",
        "                _ = model(images)\n",
        "                total_times.append(time.time() - start_time)\n",
        "\n",
        "                # ✅ Track max VRAM usage\n",
        "                max_vram = max(max_vram, get_memory_usage())\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "    throughput = len(dataset) / sum(total_times) if sum(total_times) > 0 else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": \"Edge AI\" if model_name in [\"mobilenet_v3_large\", \"efficientnet_b0\", \"squeezenet1_1\", \"shufflenet_v2_x0_5\", \"yolov8n\"] else \"High-Performance\",\n",
        "        \"Model Size (MB)\": round(model_size, 2),\n",
        "        \"FLOPs (G)\": round(macs / 1e9, 2),\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all models\n",
        "results = []\n",
        "for model_name in models_to_test:\n",
        "    results.append(evaluate_model(model_name))\n",
        "\n",
        "# ✅ Display results in a table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# ✅ Save results as CSV\n",
        "df_results.to_csv(\"model_performance_results.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XKr1ZR11l7x",
        "outputId": "4cfec86b-b7b8-4d37-867b-65b68603ffce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
            "\n",
            "🚀 Evaluating mobilenet_v3_large on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating efficientnet_b0 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating squeezenet1_1 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating shufflenet_v2_x0_5 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnet50 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 168MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating vit_b_16 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 165MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Tiny_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating convnext_tiny on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n",
            "100%|██████████| 109M/109M [00:01<00:00, 105MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Model Performance Summary:\n",
            "                Model          Category  Model Size (MB)  FLOPs (G)  \\\n",
            "0  mobilenet_v3_large           Edge AI            20.92       0.23   \n",
            "1     efficientnet_b0           Edge AI            20.17       0.42   \n",
            "2       squeezenet1_1           Edge AI             4.71       0.35   \n",
            "3  shufflenet_v2_x0_5           Edge AI             5.21       0.04   \n",
            "4            resnet50  High-Performance            97.49       4.13   \n",
            "5            vit_b_16  High-Performance           330.23      11.29   \n",
            "6       convnext_tiny  High-Performance           109.06       4.46   \n",
            "\n",
            "   Avg Inference Time (s)  Min Inference Time (s)  Max Inference Time (s)  \\\n",
            "0                  0.0222                  0.0073                  0.1213   \n",
            "1                  0.0269                  0.0093                  0.1031   \n",
            "2                  0.0066                  0.0027                  0.0764   \n",
            "3                  0.0223                  0.0073                  0.0620   \n",
            "4                  0.0210                  0.0066                  0.1070   \n",
            "5                  0.0248                  0.0112                  0.1193   \n",
            "6                  0.0335                  0.0101                  0.0784   \n",
            "\n",
            "      FPS  Throughput (images/sec)  Max VRAM Usage (MB)  \n",
            "0   45.07                    71.77                48.77  \n",
            "1   37.22                    59.26                36.32  \n",
            "2  150.64                   239.87                27.46  \n",
            "3   44.82                    71.36                19.57  \n",
            "4   47.71                    75.97               117.95  \n",
            "5   40.39                    64.32               360.90  \n",
            "6   29.86                    47.54               128.93  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from thop import profile  # FLOPs Calculation\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLO Model for Edge AI\n",
        "\n",
        "# ✅ Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ Load Pascal VOC Classification Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.VOCSegmentation(  # Use segmentation for consistent sizes\n",
        "    root=\"./data\", year=\"2012\", image_set=\"val\", download=True, transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=lambda x: (torch.stack([i[0] for i in x]), None))\n",
        "\n",
        "# ✅ Models to evaluate (EDGE + HIGH-PERFORMANCE)\n",
        "models_to_test = [\n",
        "    # EDGE AI (Optimized for speed & efficiency)\n",
        "    \"mobilenet_v2\", \"efficientnet_b1\", \"mnasnet1_0\", \"ghostnet\", \"regnet_x_400mf\",\n",
        "\n",
        "    # HIGH-PERFORMANCE (For stronger accuracy, but still efficient)\n",
        "    \"resnet18\", \"resnext50_32x4d\", \"vit_s_16\", \"swin_t\", \"convnext_small\"\n",
        "]\n",
        "\n",
        "# ✅ Function to evaluate a model\n",
        "def evaluate_model(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name} on Pascal VOC...\")\n",
        "\n",
        "    # ✅ Load pre-trained model\n",
        "    if \"yolo\" in model_name:\n",
        "        model = YOLO(f\"{model_name}.pt\").to(device)  # Load YOLO Model\n",
        "    else:\n",
        "        model_func = getattr(torchvision.models, model_name, None)\n",
        "        if model_func is None:\n",
        "            print(f\"❌ Model {model_name} is not available in torchvision.\")\n",
        "            return None\n",
        "\n",
        "        model = model_func(pretrained=True).to(device)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # ✅ Compute FLOPs and Model Size (For non-YOLO models)\n",
        "    if \"yolo\" not in model_name:\n",
        "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "        try:\n",
        "            macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "            model_size = (sum(p.numel() for p in model.parameters()) * 4) / (1024 ** 2)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not compute FLOPs for {model_name}: {e}\")\n",
        "            macs, model_size = 0, 0\n",
        "    else:\n",
        "        macs, model_size = 0, 0  # Skip FLOPs calculation for YOLO\n",
        "\n",
        "    # ✅ Performance tracking\n",
        "    total_times = []\n",
        "    max_vram = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_trials):\n",
        "            for images, _ in tqdm(dataloader, desc=f\"Running {model_name}\", leave=False):\n",
        "                images = images.to(device)\n",
        "\n",
        "                # ✅ Measure inference time\n",
        "                start_time = time.time()\n",
        "                _ = model(images)\n",
        "                total_times.append(time.time() - start_time)\n",
        "\n",
        "                # ✅ Track max VRAM usage\n",
        "                max_vram = max(max_vram, get_memory_usage())\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "    throughput = len(dataset) / sum(total_times) if sum(total_times) > 0 else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # ✅ Determine model category dynamically\n",
        "    category = \"Edge AI\" if model_name in [\"mobilenet_v2\", \"efficientnet_b1\", \"mnasnet1_0\", \"ghostnet\", \"regnet_x_400mf\"] else \"High-Performance\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": category,\n",
        "        \"Model Size (MB)\": round(model_size, 2),\n",
        "        \"FLOPs (G)\": round(macs / 1e9, 2),\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all models\n",
        "results = []\n",
        "for model_name in models_to_test:\n",
        "    results.append(evaluate_model(model_name))\n",
        "\n",
        "# ✅ Display results in a table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# ✅ Save results as CSV\n",
        "df_results.to_csv(\"model_performance_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ou5xdmIS3lv4",
        "outputId": "1f91fecc-3acc-41fd-df59-8773b8f35f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
            "\n",
            "🚀 Evaluating mobilenet_v2 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:01<00:00, 9.90MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B1_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B1_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating efficientnet_b1 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-bac287d4.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-bac287d4.pth\n",
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 41.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating mnasnet1_0 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MNASNet1_0_Weights.IMAGENET1K_V1`. You can also use `weights=MNASNet1_0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth\" to /root/.cache/torch/hub/checkpoints/mnasnet1.0_top1_73.512-f206786ef8.pth\n",
            "100%|██████████| 16.9M/16.9M [00:00<00:00, 70.8MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RegNet_X_400MF_Weights.IMAGENET1K_V1`. You can also use `weights=RegNet_X_400MF_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating ghostnet on Pascal VOC...\n",
            "❌ Model ghostnet is not available in torchvision.\n",
            "\n",
            "🚀 Evaluating regnet_x_400mf on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/regnet_x_400mf-adf1edd5.pth\" to /root/.cache/torch/hub/checkpoints/regnet_x_400mf-adf1edd5.pth\n",
            "100%|██████████| 21.3M/21.3M [00:00<00:00, 47.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnet18 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 109MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnext50_32x4d on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-7cdf4587.pth\n",
            "100%|██████████| 95.8M/95.8M [00:00<00:00, 116MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Swin_T_Weights.IMAGENET1K_V1`. You can also use `weights=Swin_T_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating vit_s_16 on Pascal VOC...\n",
            "❌ Model vit_s_16 is not available in torchvision.\n",
            "\n",
            "🚀 Evaluating swin_t on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
            "100%|██████████| 108M/108M [00:00<00:00, 153MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating convnext_small on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Small_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/convnext_small-0c510722.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small-0c510722.pth\n",
            "100%|██████████| 192M/192M [00:01<00:00, 174MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'keys'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0574c74c8f64>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# ✅ Display results in a table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n📊 Model Performance Summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    852\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_dict_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_series_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mpre_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \"\"\"\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mpre_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorchcv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5aBZY6k7vvK",
        "outputId": "2e3baeb7-c215-46f6-b01a-19c6ece59eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorchcv\n",
            "  Downloading pytorchcv-0.0.73-py2.py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorchcv) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pytorchcv) (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorchcv) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pytorchcv) (0.20.1+cu124)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pytorchcv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pytorchcv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pytorchcv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pytorchcv) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorchcv) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorchcv) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pytorchcv) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorchcv) (3.0.2)\n",
            "Downloading pytorchcv-0.0.73-py2.py3-none-any.whl (585 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/585.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.2/585.2 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorchcv\n",
            "Successfully installed pytorchcv-0.0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from thop import profile  # FLOPs Calculation\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLO Model for Edge AI\n",
        "\n",
        "# ✅ Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ VRAM Usage Tracking\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
        "    return 0  # Return 0 if using CPU\n",
        "\n",
        "# ✅ Load Pascal VOC Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.VOCSegmentation(\n",
        "    root=\"./data\", year=\"2012\", image_set=\"val\", download=True, transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=lambda x: (torch.stack([i[0] for i in x]), None))\n",
        "\n",
        "# ✅ Models to evaluate (EDGE + HIGH-PERFORMANCE)\n",
        "models_to_test = [\n",
        "    # EDGE AI (Optimized for speed & efficiency)\n",
        "    \"mobilenet_v2\", \"efficientnet_b1\", \"mnasnet1_0\", \"regnet_x_400mf\",\n",
        "\n",
        "    # HIGH-PERFORMANCE (For stronger accuracy, but still efficient)\n",
        "    \"resnet18\", \"resnext50_32x4d\", \"vit_s_16\", \"swin_t\", \"convnext_small\"\n",
        "]\n",
        "\n",
        "# ✅ Function to evaluate a model\n",
        "def evaluate_model(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name} on Pascal VOC...\")\n",
        "\n",
        "    # ✅ Load pre-trained model\n",
        "    try:\n",
        "        if \"yolo\" in model_name:\n",
        "            model = YOLO(f\"{model_name}.pt\").to(device)  # Load YOLO Model\n",
        "        else:\n",
        "            model_func = getattr(torchvision.models, model_name, None)\n",
        "            if model_func is None:\n",
        "                print(f\"❌ Model {model_name} is not available in torchvision.\")\n",
        "                return None\n",
        "\n",
        "            model = model_func(weights=\"DEFAULT\").to(device)  # Updated weight loading\n",
        "\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading {model_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # ✅ Compute FLOPs and Model Size (For non-YOLO models)\n",
        "    if \"yolo\" not in model_name:\n",
        "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "        try:\n",
        "            macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "            model_size = (sum(p.numel() for p in model.parameters()) * 4) / (1024 ** 2)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not compute FLOPs for {model_name}: {e}\")\n",
        "            macs, model_size = 0, 0\n",
        "    else:\n",
        "        macs, model_size = 0, 0  # Skip FLOPs calculation for YOLO\n",
        "\n",
        "    # ✅ Performance tracking\n",
        "    total_times = []\n",
        "    max_vram = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_trials):\n",
        "            for images, _ in tqdm(dataloader, desc=f\"Running {model_name}\", leave=False):\n",
        "                images = images.to(device)\n",
        "\n",
        "                # ✅ Measure inference time\n",
        "                start_time = time.time()\n",
        "                _ = model(images)\n",
        "                total_times.append(time.time() - start_time)\n",
        "\n",
        "                # ✅ Track max VRAM usage\n",
        "                max_vram = max(max_vram, get_memory_usage())\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "    throughput = len(dataset) / sum(total_times) if sum(total_times) > 0 else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # ✅ Determine model category dynamically\n",
        "    category = \"Edge AI\" if model_name in [\"mobilenet_v2\", \"efficientnet_b1\", \"mnasnet1_0\", \"regnet_x_400mf\"] else \"High-Performance\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": category,\n",
        "        \"Model Size (MB)\": round(model_size, 2),\n",
        "        \"FLOPs (G)\": round(macs / 1e9, 2),\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all models\n",
        "results = []\n",
        "for model_name in models_to_test:\n",
        "    result = evaluate_model(model_name)\n",
        "    if result:  # Only append if evaluation was successful\n",
        "        results.append(result)\n",
        "\n",
        "# ✅ Display results in a table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# ✅ Save results as CSV\n",
        "df_results.to_csv(\"model_performance_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA2kqg717zCp",
        "outputId": "b2c12d30-0853-4c13-faae-21e85fd3a888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-7ebf99e0.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-7ebf99e0.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating mobilenet_v2 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 68.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating efficientnet_b1 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 99.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating mnasnet1_0 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/regnet_x_400mf-62229a5f.pth\" to /root/.cache/torch/hub/checkpoints/regnet_x_400mf-62229a5f.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating regnet_x_400mf on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21.3M/21.3M [00:00<00:00, 134MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnet18 on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnext50_32x4d on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-1a0047aa.pth\n",
            "100%|██████████| 95.8M/95.8M [00:00<00:00, 171MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating vit_s_16 on Pascal VOC...\n",
            "❌ Model vit_s_16 is not available in torchvision.\n",
            "\n",
            "🚀 Evaluating swin_t on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating convnext_small on Pascal VOC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Model Performance Summary:\n",
            "             Model          Category  Model Size (MB)  FLOPs (G)  \\\n",
            "0     mobilenet_v2           Edge AI            13.37       0.33   \n",
            "1  efficientnet_b1           Edge AI            29.73       0.61   \n",
            "2       mnasnet1_0           Edge AI            16.72       0.34   \n",
            "3   regnet_x_400mf           Edge AI            20.97       0.43   \n",
            "4         resnet18  High-Performance            44.59       1.82   \n",
            "5  resnext50_32x4d  High-Performance            95.48       4.29   \n",
            "6           swin_t  High-Performance           107.91       2.98   \n",
            "7   convnext_small  High-Performance           191.59       8.70   \n",
            "\n",
            "   Avg Inference Time (s)  Min Inference Time (s)  Max Inference Time (s)  \\\n",
            "0                  0.0178                  0.0061                  0.0819   \n",
            "1                  0.0351                  0.0124                  0.0974   \n",
            "2                  0.0168                  0.0055                  0.0480   \n",
            "3                  0.0329                  0.0091                  0.0723   \n",
            "4                  0.0085                  0.0029                  0.0347   \n",
            "5                  0.0287                  0.0089                  0.0603   \n",
            "6                  0.0480                  0.0212                  0.1346   \n",
            "7                  0.0449                  0.0195                  0.1108   \n",
            "\n",
            "      FPS  Throughput (images/sec)  Max VRAM Usage (MB)  \n",
            "0   56.10                    89.33               680.48  \n",
            "1   28.49                    45.36               680.48  \n",
            "2   59.37                    94.54               680.48  \n",
            "3   30.44                    48.47               680.48  \n",
            "4  117.80                   187.57               680.48  \n",
            "5   34.82                    55.45               680.48  \n",
            "6   20.83                    33.17               680.48  \n",
            "7   22.29                    35.50               680.48  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLOv8\n",
        "import os\n",
        "\n",
        "# ✅ Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ VRAM Usage Tracking\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
        "    return 0  # Return 0 if using CPU\n",
        "\n",
        "# ✅ YOLO Model Variants\n",
        "yolo_models = [\n",
        "    # ✅ YOLOv5 Variants\n",
        "    \"yolov5s\", \"yolov5m\", \"yolov5l\", \"yolov5x\",\n",
        "\n",
        "    # ✅ YOLOv6 Variants\n",
        "    \"yolov6n\", \"yolov6s\", \"yolov6m\", \"yolov6l\",\n",
        "\n",
        "    # ✅ YOLOv7 Variants\n",
        "    \"yolov7\", \"yolov7-tiny\", \"yolov7x\",\n",
        "\n",
        "    # ✅ YOLOv8 Variants\n",
        "    \"yolov8n\", \"yolov8s\", \"yolov8m\", \"yolov8l\"\n",
        "]\n",
        "\n",
        "# ✅ Function to evaluate YOLO models\n",
        "def evaluate_yolo(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name}...\")\n",
        "\n",
        "    # ✅ Load YOLO Model\n",
        "    try:\n",
        "        model = YOLO(f\"{model_name}.pt\").to(device)  # Load Model\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading {model_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # ✅ Performance tracking\n",
        "    total_times = []\n",
        "    max_vram = 0\n",
        "\n",
        "    dummy_input = torch.randn(1, 3, 640, 640).to(device)  # YOLO expects 640x640\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_trials):\n",
        "            # ✅ Measure inference time\n",
        "            start_time = time.time()\n",
        "            _ = model(dummy_input)  # Run inference\n",
        "            total_times.append(time.time() - start_time)\n",
        "\n",
        "            # ✅ Track max VRAM usage\n",
        "            max_vram = max(max_vram, get_memory_usage())\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all YOLO models\n",
        "results = []\n",
        "for model_name in yolo_models:\n",
        "    result = evaluate_yolo(model_name)\n",
        "    if result:  # Only append if evaluation was successful\n",
        "        results.append(result)\n",
        "\n",
        "# ✅ Display results in a table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 YOLO Model Performance Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# ✅ Save results as CSV\n",
        "df_results.to_csv(\"yolo_performance_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxs3jOsa-eRm",
        "outputId": "84c11eaf-95e0-4a0a-b782-370e23d2c277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "🚀 Evaluating yolov5s...\n",
            "PRO TIP 💡 Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt to 'yolov5su.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.7M/17.7M [00:00<00:00, 123MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.207490921020508. Dividing input by 255.\n",
            "0: 640x640 (no detections), 15.0ms\n",
            "Speed: 0.1ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.207490921020508. Dividing input by 255.\n",
            "0: 640x640 (no detections), 15.0ms\n",
            "Speed: 0.0ms preprocess, 15.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.207490921020508. Dividing input by 255.\n",
            "0: 640x640 (no detections), 15.3ms\n",
            "Speed: 0.0ms preprocess, 15.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.207490921020508. Dividing input by 255.\n",
            "0: 640x640 (no detections), 15.0ms\n",
            "Speed: 0.0ms preprocess, 15.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.207490921020508. Dividing input by 255.\n",
            "0: 640x640 (no detections), 14.7ms\n",
            "Speed: 0.0ms preprocess, 14.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov5m...\n",
            "PRO TIP 💡 Replace 'model=yolov5m.pt' with new 'model=yolov5mu.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5mu.pt to 'yolov5mu.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 48.2M/48.2M [00:00<00:00, 109MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.080650329589844. Dividing input by 255.\n",
            "0: 640x640 (no detections), 32.3ms\n",
            "Speed: 0.0ms preprocess, 32.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.080650329589844. Dividing input by 255.\n",
            "0: 640x640 (no detections), 32.4ms\n",
            "Speed: 0.0ms preprocess, 32.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.080650329589844. Dividing input by 255.\n",
            "0: 640x640 (no detections), 32.4ms\n",
            "Speed: 0.2ms preprocess, 32.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.080650329589844. Dividing input by 255.\n",
            "0: 640x640 (no detections), 29.9ms\n",
            "Speed: 0.2ms preprocess, 29.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.080650329589844. Dividing input by 255.\n",
            "0: 640x640 (no detections), 19.4ms\n",
            "Speed: 0.2ms preprocess, 19.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov5l...\n",
            "PRO TIP 💡 Replace 'model=yolov5l.pt' with new 'model=yolov5lu.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5lu.pt to 'yolov5lu.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102M/102M [00:01<00:00, 86.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.795440196990967. Dividing input by 255.\n",
            "0: 640x640 (no detections), 53.4ms\n",
            "Speed: 0.0ms preprocess, 53.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.795440196990967. Dividing input by 255.\n",
            "0: 640x640 (no detections), 49.9ms\n",
            "Speed: 0.0ms preprocess, 49.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.795440196990967. Dividing input by 255.\n",
            "0: 640x640 (no detections), 49.3ms\n",
            "Speed: 0.0ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.795440196990967. Dividing input by 255.\n",
            "0: 640x640 (no detections), 49.1ms\n",
            "Speed: 0.0ms preprocess, 49.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.795440196990967. Dividing input by 255.\n",
            "0: 640x640 (no detections), 35.8ms\n",
            "Speed: 0.0ms preprocess, 35.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov5x...\n",
            "PRO TIP 💡 Replace 'model=yolov5x.pt' with new 'model=yolov5xu.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5xu.pt to 'yolov5xu.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 186M/186M [00:01<00:00, 108MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.928525447845459. Dividing input by 255.\n",
            "0: 640x640 (no detections), 99.1ms\n",
            "Speed: 0.0ms preprocess, 99.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.928525447845459. Dividing input by 255.\n",
            "0: 640x640 (no detections), 76.8ms\n",
            "Speed: 0.0ms preprocess, 76.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.928525447845459. Dividing input by 255.\n",
            "0: 640x640 (no detections), 61.1ms\n",
            "Speed: 0.0ms preprocess, 61.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.928525447845459. Dividing input by 255.\n",
            "0: 640x640 (no detections), 60.9ms\n",
            "Speed: 0.0ms preprocess, 60.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.928525447845459. Dividing input by 255.\n",
            "0: 640x640 (no detections), 62.4ms\n",
            "Speed: 0.0ms preprocess, 62.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov6n...\n",
            "⚠️ Error loading yolov6n: [Errno 2] No such file or directory: 'yolov6n.pt'\n",
            "\n",
            "🚀 Evaluating yolov6s...\n",
            "⚠️ Error loading yolov6s: [Errno 2] No such file or directory: 'yolov6s.pt'\n",
            "\n",
            "🚀 Evaluating yolov6m...\n",
            "⚠️ Error loading yolov6m: [Errno 2] No such file or directory: 'yolov6m.pt'\n",
            "\n",
            "🚀 Evaluating yolov6l...\n",
            "⚠️ Error loading yolov6l: [Errno 2] No such file or directory: 'yolov6l.pt'\n",
            "\n",
            "🚀 Evaluating yolov7...\n",
            "⚠️ Error loading yolov7: [Errno 2] No such file or directory: 'yolov7.pt'\n",
            "\n",
            "🚀 Evaluating yolov7-tiny...\n",
            "⚠️ Error loading yolov7-tiny: [Errno 2] No such file or directory: 'yolov7-tiny.pt'\n",
            "\n",
            "🚀 Evaluating yolov7x...\n",
            "⚠️ Error loading yolov7x: [Errno 2] No such file or directory: 'yolov7x.pt'\n",
            "\n",
            "🚀 Evaluating yolov8n...\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.985334873199463. Dividing input by 255.\n",
            "0: 640x640 (no detections), 7.9ms\n",
            "Speed: 0.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.985334873199463. Dividing input by 255.\n",
            "0: 640x640 (no detections), 8.4ms\n",
            "Speed: 0.0ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.985334873199463. Dividing input by 255.\n",
            "0: 640x640 (no detections), 8.4ms\n",
            "Speed: 0.0ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.985334873199463. Dividing input by 255.\n",
            "0: 640x640 (no detections), 8.5ms\n",
            "Speed: 0.0ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.985334873199463. Dividing input by 255.\n",
            "0: 640x640 (no detections), 8.5ms\n",
            "Speed: 0.1ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov8s...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 21.5M/21.5M [00:00<00:00, 209MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.758820056915283. Dividing input by 255.\n",
            "0: 640x640 (no detections), 16.1ms\n",
            "Speed: 0.0ms preprocess, 16.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.758820056915283. Dividing input by 255.\n",
            "0: 640x640 (no detections), 16.1ms\n",
            "Speed: 0.0ms preprocess, 16.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.758820056915283. Dividing input by 255.\n",
            "0: 640x640 (no detections), 16.1ms\n",
            "Speed: 0.0ms preprocess, 16.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.758820056915283. Dividing input by 255.\n",
            "0: 640x640 (no detections), 16.1ms\n",
            "Speed: 0.0ms preprocess, 16.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.758820056915283. Dividing input by 255.\n",
            "0: 640x640 (no detections), 16.1ms\n",
            "Speed: 0.0ms preprocess, 16.1ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov8m...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 296MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.142816543579102. Dividing input by 255.\n",
            "0: 640x640 (no detections), 36.5ms\n",
            "Speed: 0.0ms preprocess, 36.5ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.142816543579102. Dividing input by 255.\n",
            "0: 640x640 (no detections), 36.6ms\n",
            "Speed: 0.0ms preprocess, 36.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.142816543579102. Dividing input by 255.\n",
            "0: 640x640 (no detections), 36.5ms\n",
            "Speed: 0.0ms preprocess, 36.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.142816543579102. Dividing input by 255.\n",
            "0: 640x640 (no detections), 36.5ms\n",
            "Speed: 0.0ms preprocess, 36.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 5.142816543579102. Dividing input by 255.\n",
            "0: 640x640 (no detections), 29.2ms\n",
            "Speed: 0.0ms preprocess, 29.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "🚀 Evaluating yolov8l...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8l.pt to 'yolov8l.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 83.7M/83.7M [00:01<00:00, 52.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.722289562225342. Dividing input by 255.\n",
            "0: 640x640 (no detections), 61.5ms\n",
            "Speed: 0.0ms preprocess, 61.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.722289562225342. Dividing input by 255.\n",
            "0: 640x640 (no detections), 61.6ms\n",
            "Speed: 0.0ms preprocess, 61.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.722289562225342. Dividing input by 255.\n",
            "0: 640x640 (no detections), 38.4ms\n",
            "Speed: 0.0ms preprocess, 38.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.722289562225342. Dividing input by 255.\n",
            "0: 640x640 (no detections), 37.2ms\n",
            "Speed: 0.0ms preprocess, 37.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "WARNING ⚠️ torch.Tensor inputs should be normalized 0.0-1.0 but max value is 4.722289562225342. Dividing input by 255.\n",
            "0: 640x640 (no detections), 37.8ms\n",
            "Speed: 0.0ms preprocess, 37.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "📊 YOLO Model Performance Summary:\n",
            "     Model  Avg Inference Time (s)  Min Inference Time (s)  \\\n",
            "0  yolov5s                  0.0557                  0.0196   \n",
            "1  yolov5m                  0.1476                  0.0257   \n",
            "2  yolov5l                  0.1638                  0.0413   \n",
            "3  yolov5x                  0.2625                  0.0664   \n",
            "4  yolov8n                  0.0326                  0.0132   \n",
            "5  yolov8s                  0.0675                  0.0213   \n",
            "6  yolov8m                  0.1437                  0.0411   \n",
            "7  yolov8l                  0.1462                  0.0433   \n",
            "\n",
            "   Max Inference Time (s)    FPS  Max VRAM Usage (MB)  \n",
            "0                  0.1965  17.96               423.81  \n",
            "1                  0.5997   6.78               423.81  \n",
            "2                  0.6102   6.10               423.81  \n",
            "3                  1.0272   3.81               680.48  \n",
            "4                  0.1064  30.63               680.48  \n",
            "5                  0.2478  14.82               680.48  \n",
            "6                  0.5419   6.96               680.48  \n",
            "7                  0.5315   6.84               680.48  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop\n",
        "!pip install pycocotools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAQEJ7B2r3xg",
        "outputId": "a4927f6e-235b-46b2-b992-07068dcb4395"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->thop)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->thop)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->thop)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->thop)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->thop)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->thop)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools) (1.26.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading coc2017 dataset**"
      ],
      "metadata": {
        "id": "LiPcRMK4t3kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# ✅ Define the COCO dataset directory in Google Drive\n",
        "coco_dir = \"/content/drive/My Drive/Dataset/COCO2017\"\n",
        "os.makedirs(coco_dir, exist_ok=True)\n",
        "\n",
        "# ✅ COCO dataset URLs\n",
        "coco_urls = {\n",
        "    \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n",
        "    \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
        "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
        "}\n",
        "\n",
        "# ✅ Function to download and extract files\n",
        "def download_and_extract(url, save_path, extract_path):\n",
        "    filename = url.split(\"/\")[-1]\n",
        "    filepath = os.path.join(save_path, filename)\n",
        "\n",
        "    # Download file if not already exists\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"⬇️ Downloading {filename}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(filepath, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "\n",
        "    # Extract file\n",
        "    print(f\"📦 Extracting {filename}...\")\n",
        "    with zipfile.ZipFile(filepath, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"✅ Extracted {filename}\")\n",
        "\n",
        "# ✅ Download & extract train, val images and annotations\n",
        "for key, url in coco_urls.items():\n",
        "    download_and_extract(url, coco_dir, coco_dir)\n",
        "\n",
        "print(\"\\n🚀 COCO 2017 dataset is stored in Google Drive at:\", coco_dir)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62PexfLit8SE",
        "outputId": "5ddec35b-0a18-4e9f-81aa-672da2882140"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading train2017.zip...\n",
            "✅ Downloaded train2017.zip\n",
            "📦 Extracting train2017.zip...\n",
            "✅ Extracted train2017.zip\n",
            "⬇️ Downloading val2017.zip...\n",
            "✅ Downloaded val2017.zip\n",
            "📦 Extracting val2017.zip...\n",
            "✅ Extracted val2017.zip\n",
            "⬇️ Downloading annotations_trainval2017.zip...\n",
            "✅ Downloaded annotations_trainval2017.zip\n",
            "📦 Extracting annotations_trainval2017.zip...\n",
            "✅ Extracted annotations_trainval2017.zip\n",
            "\n",
            "🚀 COCO 2017 dataset is stored in Google Drive at: /content/drive/My Drive/Dataset/COCO2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying various olo models with coco 2017 data set"
      ],
      "metadata": {
        "id": "5oGad9uarLkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from thop import profile  # FLOPs Calculation\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLO Model for Edge AI\n",
        "from pycocotools.coco import COCO\n",
        "from torchvision.datasets import CocoDetection\n",
        "import os\n",
        "\n",
        "# ✅ Set device (Use T4 GPU on Google Colab)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ VRAM Usage Tracking\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
        "    return 0  # Return 0 if using CPU\n",
        "\n",
        "# ✅ Define dataset paths (Stored Locally)\n",
        "data_root = \"/content/drive/MyDrive/Dataset/COCO2017\"\n",
        "ann_file = os.path.join(data_root, \"annotations/instances_val2017.json\")\n",
        "img_dir = os.path.join(data_root, \"val2017\")\n",
        "\n",
        "# ✅ Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# ✅ Load COCO 2017 Dataset\n",
        "dataset = CocoDetection(root=img_dir, annFile=ann_file, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=lambda x: (torch.stack([i[0] for i in x]), None))\n",
        "\n",
        "# ✅ Models to evaluate\n",
        "models_to_test = [\n",
        "    \"mobilenet_v2\", \"efficientnet_b1\", \"mnasnet1_0\", \"regnet_x_400mf\",  # EDGE AI\n",
        "    \"resnet18\", \"resnext50_32x4d\", \"vit_s_16\", \"swin_t\", \"convnext_small\"  # HIGH-PERFORMANCE\n",
        "]\n",
        "\n",
        "# ✅ Function to evaluate a model\n",
        "def evaluate_model(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name} on COCO 2017...\")\n",
        "    try:\n",
        "        if \"yolo\" in model_name:\n",
        "            model = YOLO(f\"{model_name}.pt\").to(device)  # Load YOLO Model\n",
        "        else:\n",
        "            model_func = getattr(torchvision.models, model_name, None)\n",
        "            if model_func is None:\n",
        "                print(f\"❌ Model {model_name} is not available in torchvision.\")\n",
        "                return None\n",
        "            model = model_func(weights=\"DEFAULT\").to(device)\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading {model_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # ✅ Compute FLOPs and Model Size (For non-YOLO models)\n",
        "    if \"yolo\" not in model_name:\n",
        "        dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "        try:\n",
        "            macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
        "            model_size = (sum(p.numel() for p in model.parameters()) * 4) / (1024 ** 2)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Could not compute FLOPs for {model_name}: {e}\")\n",
        "            macs, model_size = 0, 0\n",
        "    else:\n",
        "        macs, model_size = 0, 0\n",
        "\n",
        "    # ✅ Performance tracking\n",
        "    total_times = []\n",
        "    max_vram = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_trials):\n",
        "            for images, _ in tqdm(dataloader, desc=f\"Running {model_name}\", leave=False):\n",
        "                images = images.to(device)\n",
        "                start_time = time.time()\n",
        "                _ = model(images)\n",
        "                total_times.append(time.time() - start_time)\n",
        "                max_vram = max(max_vram, get_memory_usage())\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "    throughput = len(dataset) / sum(total_times) if sum(total_times) > 0 else 0\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    category = \"Edge AI\" if model_name in [\"mobilenet_v2\", \"efficientnet_b1\", \"mnasnet1_0\", \"regnet_x_400mf\"] else \"High-Performance\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": category,\n",
        "        \"Model Size (MB)\": round(model_size, 2),\n",
        "        \"FLOPs (G)\": round(macs / 1e9, 2),\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all models\n",
        "results = []\n",
        "for model_name in models_to_test:\n",
        "    result = evaluate_model(model_name)\n",
        "    if result:\n",
        "        results.append(result)\n",
        "\n",
        "# ✅ Display results in a table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# ✅ Save results as CSV\n",
        "df_results.to_csv(\"coco_model_performance.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EUBvkggrRWw",
        "outputId": "9a38d092-d932-450c-ed1e-ac54323e0bdb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "loading annotations into memory...\n",
            "Done (t=1.03s)\n",
            "creating index...\n",
            "index created!\n",
            "\n",
            "🚀 Evaluating mobilenet_v2 on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating efficientnet_b1 on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b1-c27df63c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b1-c27df63c.pth\n",
            "100%|██████████| 30.1M/30.1M [00:00<00:00, 101MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth\" to /root/.cache/torch/hub/checkpoints/mnasnet1.0_top1_73.512-f206786ef8.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating mnasnet1_0 on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16.9M/16.9M [00:00<00:00, 83.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating regnet_x_400mf on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/regnet_x_400mf-62229a5f.pth\" to /root/.cache/torch/hub/checkpoints/regnet_x_400mf-62229a5f.pth\n",
            "100%|██████████| 21.3M/21.3M [00:00<00:00, 30.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnet18 on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 72.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating resnext50_32x4d on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-1a0047aa.pth\n",
            "100%|██████████| 95.8M/95.8M [00:01<00:00, 82.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating vit_s_16 on COCO 2017...\n",
            "❌ Model vit_s_16 is not available in torchvision.\n",
            "\n",
            "🚀 Evaluating swin_t on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n",
            "100%|██████████| 108M/108M [00:01<00:00, 78.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Evaluating convnext_small on COCO 2017...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_small-0c510722.pth\" to /root/.cache/torch/hub/checkpoints/convnext_small-0c510722.pth\n",
            "100%|██████████| 192M/192M [00:01<00:00, 147MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Model Performance Summary:\n",
            "             Model          Category  Model Size (MB)  FLOPs (G)  \\\n",
            "0     mobilenet_v2           Edge AI            13.37       0.33   \n",
            "1  efficientnet_b1           Edge AI            29.73       0.61   \n",
            "2       mnasnet1_0           Edge AI            16.72       0.34   \n",
            "3   regnet_x_400mf           Edge AI            20.97       0.43   \n",
            "4         resnet18  High-Performance            44.59       1.82   \n",
            "5  resnext50_32x4d  High-Performance            95.48       4.29   \n",
            "6           swin_t  High-Performance           107.91       2.98   \n",
            "7   convnext_small  High-Performance           191.59       8.70   \n",
            "\n",
            "   Avg Inference Time (s)  Min Inference Time (s)  Max Inference Time (s)  \\\n",
            "0                  0.0171                  0.0062                  0.2239   \n",
            "1                  0.0373                  0.0126                  0.1497   \n",
            "2                  0.0162                  0.0056                  0.0688   \n",
            "3                  0.0328                  0.0108                  0.1491   \n",
            "4                  0.0078                  0.0028                  0.0592   \n",
            "5                  0.0277                  0.0095                  0.1154   \n",
            "6                  0.0358                  0.0131                  0.1405   \n",
            "7                  0.0317                  0.0111                  0.1153   \n",
            "\n",
            "      FPS  Throughput (images/sec)  Max VRAM Usage (MB)  \n",
            "0   58.39                    93.43               106.50  \n",
            "1   26.83                    42.92               123.50  \n",
            "2   61.62                    98.59               123.50  \n",
            "3   30.53                    48.84               123.50  \n",
            "4  128.80                   206.08               123.50  \n",
            "5   36.07                    57.71               190.31  \n",
            "6   27.92                    44.67               232.00  \n",
            "7   31.50                    50.40               298.76  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "# ✅ Define COCO dataset directory in Google Drive\n",
        "coco_dir = \"/content/drive/My Drive/Dataset/COCO2017\"\n",
        "os.makedirs(coco_dir, exist_ok=True)\n",
        "\n",
        "# ✅ COCO dataset URLs and corresponding folders\n",
        "coco_data = {\n",
        "    \"val_images\": {\n",
        "        \"url\": \"http://images.cocodataset.org/zips/val2017.zip\",\n",
        "        \"folder\": os.path.join(coco_dir, \"val2017\")\n",
        "    },\n",
        "    \"annotations\": {\n",
        "        \"url\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\",\n",
        "        \"folder\": os.path.join(coco_dir, \"annotations\")\n",
        "    },\n",
        "}\n",
        "\n",
        "# ✅ Function to download and extract files\n",
        "def download_and_extract(url, save_path, extract_path):\n",
        "    filename = url.split(\"/\")[-1]\n",
        "    filepath = os.path.join(save_path, filename)\n",
        "\n",
        "    # Download file if not already exists\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"⬇️ Downloading {filename}...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(filepath, \"wb\") as file:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                file.write(chunk)\n",
        "        print(f\"✅ Downloaded {filename}\")\n",
        "\n",
        "    # Extract file\n",
        "    print(f\"📦 Extracting {filename}...\")\n",
        "    with zipfile.ZipFile(filepath, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"✅ Extracted {filename}\")\n",
        "\n",
        "# ✅ Check and download only missing folders (Skip train2017)\n",
        "for key, data in coco_data.items():\n",
        "    folder_path = data[\"folder\"]\n",
        "\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"\\n🚀 {folder_path} is missing. Downloading...\")\n",
        "        download_and_extract(data[\"url\"], coco_dir, coco_dir)\n",
        "    else:\n",
        "        print(f\"✅ {folder_path} already exists. Skipping download.\")\n",
        "\n",
        "print(\"\\n🎉 COCO 2017 dataset is fully available in Google Drive at:\", coco_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooq0fAVJPdkz",
        "outputId": "fee84aee-e3c5-497a-fe62-67cef2b52822"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 /content/drive/My Drive/Dataset/COCO2017/val2017 is missing. Downloading...\n",
            "⬇️ Downloading val2017.zip...\n",
            "✅ Downloaded val2017.zip\n",
            "📦 Extracting val2017.zip...\n",
            "✅ Extracted val2017.zip\n",
            "\n",
            "🚀 /content/drive/My Drive/Dataset/COCO2017/annotations is missing. Downloading...\n",
            "⬇️ Downloading annotations_trainval2017.zip...\n",
            "✅ Downloaded annotations_trainval2017.zip\n",
            "📦 Extracting annotations_trainval2017.zip...\n",
            "✅ Extracted annotations_trainval2017.zip\n",
            "\n",
            "🎉 COCO 2017 dataset is fully available in Google Drive at: /content/drive/My Drive/Dataset/COCO2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running Yolo models on COCO2017 Dataset**"
      ],
      "metadata": {
        "id": "BaCkJhYDNZTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import gc\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from ultralytics import YOLO  # YOLOv8\n",
        "from torchinfo import summary  # Alternative to `thop`\n",
        "\n",
        "# ✅ Define device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ VRAM Usage Tracking\n",
        "def get_memory_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / (1024 ** 2)  # Convert bytes to MB\n",
        "    return 0  # Return 0 if using CPU\n",
        "\n",
        "# ✅ YOLO Model Variants\n",
        "yolo_models = {\n",
        "    \"yolov5s\": \"yolov5s.pt\",\n",
        "    \"yolov5m\": \"yolov5m.pt\",\n",
        "    \"yolov5l\": \"yolov5l.pt\",\n",
        "    \"yolov5x\": \"yolov5x.pt\",\n",
        "    \"yolov8n\": \"yolov8n.pt\",\n",
        "    \"yolov8s\": \"yolov8s.pt\",\n",
        "    \"yolov8m\": \"yolov8m.pt\",\n",
        "    \"yolov8l\": \"yolov8l.pt\",\n",
        "}\n",
        "\n",
        "# ✅ Function to compute FLOPs and Model Size\n",
        "def compute_flops_and_size(model):\n",
        "    try:\n",
        "        dummy_input = torch.randn(1, 3, 640, 640).to(device)  # Dummy input\n",
        "\n",
        "        # ✅ Compute Model Size (MB)\n",
        "        model_size = sum(p.numel() for p in model.parameters()) * 4 / (1024 ** 2)  # MB\n",
        "\n",
        "        # ✅ Compute FLOPs using `torchinfo.summary()`\n",
        "        model_summary = summary(model, input_size=(1, 3, 640, 640), verbose=0)\n",
        "        total_flops = model_summary.total_mult_adds / 1e9  # Convert to GFLOPs\n",
        "\n",
        "        return round(model_size, 2), round(total_flops, 2)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not compute FLOPs for model: {e}\")\n",
        "        return 0, 0\n",
        "\n",
        "# ✅ Function to evaluate YOLO models\n",
        "def evaluate_yolo(model_name, num_trials=5):\n",
        "    print(f\"\\n🚀 Evaluating {model_name}...\")\n",
        "\n",
        "    # ✅ Load YOLO Model (Reinitialize each time)\n",
        "    try:\n",
        "        model = YOLO(yolo_models[model_name]).to(device)  # Load & move to CUDA\n",
        "        model.eval()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error loading {model_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "    # ✅ Compute FLOPs & Model Size\n",
        "    model_size, flops = compute_flops_and_size(model)\n",
        "\n",
        "    # ✅ Performance tracking\n",
        "    total_times = []\n",
        "    max_vram = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_trials):\n",
        "            dummy_input = torch.randn(1, 3, 640, 640).to(device) / 255.0  # Normalize & move to CUDA\n",
        "            start_time = time.time()\n",
        "            _ = model(dummy_input)  # Run inference\n",
        "            total_times.append(time.time() - start_time)\n",
        "\n",
        "            # ✅ Track max VRAM usage\n",
        "            max_vram = max(max_vram, get_memory_usage())\n",
        "\n",
        "    # ✅ Compute final metrics\n",
        "    avg_time = np.mean(total_times)\n",
        "    min_time, max_time = np.min(total_times), np.max(total_times)\n",
        "    fps = 1 / avg_time if avg_time > 0 else 0\n",
        "    throughput = num_trials / avg_time if avg_time > 0 else 0\n",
        "\n",
        "    # ✅ Clear GPU memory for next model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # ✅ Determine model category (Edge AI vs High-Performance)\n",
        "    edge_models = [\"yolov5s\", \"yolov8n\", \"yolov5m\", \"yolov8s\"]\n",
        "    category = \"Edge AI\" if model_name in edge_models else \"High-Performance\"\n",
        "\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Category\": category,\n",
        "        \"Model Size (MB)\": model_size,\n",
        "        \"FLOPs (G)\": flops,\n",
        "        \"Avg Inference Time (s)\": round(avg_time, 4),\n",
        "        \"Min Inference Time (s)\": round(min_time, 4),\n",
        "        \"Max Inference Time (s)\": round(max_time, 4),\n",
        "        \"FPS\": round(fps, 2),\n",
        "        \"Throughput (images/sec)\": round(throughput, 2),\n",
        "        \"Max VRAM Usage (MB)\": round(max_vram, 2),\n",
        "    }\n",
        "\n",
        "# ✅ Run evaluation for all YOLO models\n",
        "results = []\n",
        "for model_name in yolo_models.keys():\n",
        "    result = evaluate_yolo(model_name)\n",
        "    if result:  # Only append if evaluation was successful\n",
        "        results.append(result)\n",
        "\n",
        "# ✅ Display results in a table\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\n📊 YOLO Model Performance Summary:\")\n",
        "print(df_results)\n",
        "\n",
        "# ✅ Save results as CSV\n",
        "df_results.to_csv(\"yolo_performance_results.csv\", index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KIr3qkfXNiRQ",
        "outputId": "a140c027-7817-4c85-a027-9452d80982bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "🚀 Evaluating yolov5s...\n",
            "PRO TIP 💡 Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
            "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
            "\n",
            "\n",
            "0: 640x640 (no detections), 30.4ms\n",
            "Speed: 0.0ms preprocess, 30.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov5s.pt, data=coco.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
            "\n",
            "Dataset 'coco.yaml' images not found ⚠️, missing path '/content/datasets/coco/val2017.txt'\n",
            "Downloading https://ultralytics.com/assets/coco2017labels-segments.zip to '/content/datasets/coco2017labels-segments.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:04<00:00, 39.0MB/s]\n",
            "Unzipping /content/datasets/coco2017labels-segments.zip to /content/datasets/coco...: 100%|██████████| 122232/122232 [00:20<00:00, 5982.28file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://images.cocodataset.org/zips/train2017.zip to '/content/datasets/coco/images/train2017.zip'...\n",
            "Downloading http://images.cocodataset.org/zips/val2017.zip to '/content/datasets/coco/images/val2017.zip'...\n",
            "Downloading http://images.cocodataset.org/zips/test2017.zip to '/content/datasets/coco/images/test2017.zip'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset download success ✅ (1398.8s), saved to \u001b[1m/content/datasets\u001b[0m\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 755k/755k [00:00<00:00, 106MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      3520  ultralytics.nn.modules.conv.Conv             [3, 32, 6, 2, 2]              \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     18816  ultralytics.nn.modules.block.C3              [64, 64, 1]                   \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    115712  ultralytics.nn.modules.block.C3              [128, 128, 2]                 \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  3    625152  ultralytics.nn.modules.block.C3              [256, 256, 3]                 \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1]                 \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1]              \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    361984  ultralytics.nn.modules.block.C3              [512, 256, 1, False]          \n",
            " 14                  -1  1     33024  ultralytics.nn.modules.conv.Conv             [256, 128, 1, 1]              \n",
            " 15                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 16             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 17                  -1  1     90880  ultralytics.nn.modules.block.C3              [256, 128, 1, False]          \n",
            " 18                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 19            [-1, 14]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 20                  -1  1    296448  ultralytics.nn.modules.block.C3              [256, 256, 1, False]          \n",
            " 21                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 22            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 23                  -1  1   1182720  ultralytics.nn.modules.block.C3              [512, 512, 1, False]          \n",
            " 24        [17, 20, 23]  1   2147008  ultralytics.nn.modules.head.Detect           [80, [128, 256, 512]]         \n",
            "YOLOv5s summary: 153 layers, 9,153,152 parameters, 9,153,136 gradients, 24.2 GFLOPs\n",
            "\n",
            "Transferred 82/427 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train', view at http://localhost:6006/\n",
            "Freezing layer 'model.24.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.35M/5.35M [00:00<00:00, 301MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco/labels/train2017... 117266 images, 1021 backgrounds, 0 corrupt: 100%|██████████| 118287/118287 [05:59<00:00, 328.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/coco/labels/train2017.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:14<00:00, 335.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/labels/val2017.cache\n",
            "Plotting labels to runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 69 weight(decay=0.0), 76 weight(decay=0.0005), 75 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      1/100      4.24G      1.656      2.704      1.558        237        640:  46%|████▌     | 3375/7393 [34:10<40:41,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-68ad33a535cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myolo_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_yolo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only append if evaluation was successful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-68ad33a535cc>\u001b[0m in \u001b[0;36mevaluate_yolo\u001b[0;34m(model_name, num_trials)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# ✅ Compute FLOPs & Model Size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmodel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_flops_and_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# ✅ Performance tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-68ad33a535cc>\u001b[0m in \u001b[0;36mcompute_flops_and_size\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# ✅ Compute FLOPs using `torchinfo.summary()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmodel_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m640\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mtotal_flops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_mult_adds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1e9\u001b[0m  \u001b[0;31m# Convert to GFLOPs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0;31m     summary_list = forward_pass(\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_forward_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchinfo/torchinfo.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mpre_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_model_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0madd_missing_container_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTQDM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_batch_start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;31m# Warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;34m\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}